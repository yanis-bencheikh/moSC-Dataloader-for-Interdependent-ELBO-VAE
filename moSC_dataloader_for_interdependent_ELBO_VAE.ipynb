{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PPTm2zpddUTU",
        "mdHOCmRldZeR",
        "y6pupaIydt32",
        "PtJveqKI6Oqc",
        "rJjNNj-dydTM",
        "r3PX51gryfYD",
        "XG9lS79myY3V"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download & Uncompress Datasets"
      ],
      "metadata": {
        "id": "PPTm2zpddUTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# import requests\n",
        "# import gzip\n",
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# # Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Define URLs and file paths\n",
        "# urls = {\n",
        "#     \"CITE-seq\": \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE194122&format=file&file=GSE194122%5Fopenproblems%5Fneurips2021%5Fcite%5FBMMC%5Fprocessed%2Eh5ad%2Egz\",\n",
        "#     \"Multiome\": \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE194122&format=file&file=GSE194122%5Fopenproblems%5Fneurips2021%5Fmultiome%5FBMMC%5Fprocessed%2Eh5ad%2Egz\"\n",
        "# }\n",
        "\n",
        "# output_dir = \"/content/drive/MyDrive/ProjectX/data\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# for name, url in urls.items():\n",
        "#     output_gz = os.path.join(output_dir, f\"{name}.h5ad.gz\")\n",
        "#     output_file = os.path.join(output_dir, f\"{name}.h5ad\")\n",
        "\n",
        "#     # Download the dataset\n",
        "#     print(f\"Downloading {name} dataset...\")\n",
        "#     response = requests.get(url, stream=True)\n",
        "#     with open(output_gz, 'wb') as f:\n",
        "#         for chunk in response.iter_content(chunk_size=1024):\n",
        "#             if chunk:\n",
        "#                 f.write(chunk)\n",
        "#     print(f\"{name} dataset saved to {output_gz}\")\n",
        "\n",
        "#     # Decompress the file\n",
        "#     print(f\"Decompressing {name} dataset...\")\n",
        "#     with gzip.open(output_gz, 'rb') as f_in:\n",
        "#         with open(output_file, 'wb') as f_out:\n",
        "#             shutil.copyfileobj(f_in, f_out)\n",
        "#     print(f\"{name} dataset decompressed and saved to {output_file}\")\n",
        "\n",
        "#     # Optional: Delete the .gz file to save space\n",
        "#     os.remove(output_gz)\n",
        "#     print(f\"Compressed file {output_gz} deleted.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a72DPzJaZl_m",
        "outputId": "b4f53b59-74ab-41bc-92f9-d674004ae990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Downloading CITE-seq dataset...\n",
            "CITE-seq dataset saved to /content/drive/MyDrive/ProjectX/data/CITE-seq.h5ad.gz\n",
            "Decompressing CITE-seq dataset...\n",
            "CITE-seq dataset decompressed and saved to /content/drive/MyDrive/ProjectX/data/CITE-seq.h5ad\n",
            "Compressed file /content/drive/MyDrive/ProjectX/data/CITE-seq.h5ad.gz deleted.\n",
            "\n",
            "Downloading Multiome dataset...\n",
            "Multiome dataset saved to /content/drive/MyDrive/ProjectX/data/Multiome.h5ad.gz\n",
            "Decompressing Multiome dataset...\n",
            "Multiome dataset decompressed and saved to /content/drive/MyDrive/ProjectX/data/Multiome.h5ad\n",
            "Compressed file /content/drive/MyDrive/ProjectX/data/Multiome.h5ad.gz deleted.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs & Loading Datasets"
      ],
      "metadata": {
        "id": "mdHOCmRldZeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8EmnrOpY_fR",
        "outputId": "4992a5cf-b453-4e24-a4c7-19d229ab2320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scanpy torch numpy pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug98nBdvZTGe",
        "outputId": "d055f1cd-8073-41ca-927a-cbc5bd03c53d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scanpy\n",
            "  Downloading scanpy-1.10.4-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Collecting anndata>=0.8 (from scanpy)\n",
            "  Downloading anndata-0.11.1-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: h5py>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.12.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.4.2)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.8.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.4.2)\n",
            "Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (24.2)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.0.1)\n",
            "Collecting pynndescent>=0.5 (from scanpy)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.1 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.13.2)\n",
            "Collecting session-info (from scanpy)\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy) (4.66.6)\n",
            "Collecting umap-learn!=0.5.0,>=0.5 (from scanpy)\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Collecting array-api-compat!=1.5,>1.4 (from anndata>=0.8->scanpy)\n",
            "  Downloading array_api_compat-1.9.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.56->scanpy) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1->scanpy) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting stdlib_list (from session-info->scanpy)\n",
            "  Downloading stdlib_list-0.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Downloading scanpy-1.10.4-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anndata-0.11.1-py3-none-any.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4-py3-none-any.whl (15 kB)\n",
            "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading array_api_compat-1.9.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stdlib_list-0.11.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: session-info\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8023 sha256=68c2b93ce52cf5361d1874a29e11bc85fe4b055102e607ef6bf118467900d743\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
            "Successfully built session-info\n",
            "Installing collected packages: stdlib_list, legacy-api-wrap, array-api-compat, session-info, pynndescent, anndata, umap-learn, scanpy\n",
            "Successfully installed anndata-0.11.1 array-api-compat-1.9.1 legacy-api-wrap-1.4 pynndescent-0.5.13 scanpy-1.10.4 session-info-1.0.0 stdlib_list-0.11.0 umap-learn-0.5.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scanpy as sc\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define the paths to the datasets\n",
        "cite_path = \"/content/drive/MyDrive/ProjectX/data/CITE-seq.h5ad\"\n",
        "multiome_path = \"/content/drive/MyDrive/ProjectX/data/Multiome.h5ad\"\n",
        "\n",
        "# Load the datasets using scanpy\n",
        "adata_cite = sc.read_h5ad(cite_path)\n",
        "adata_multiome = sc.read_h5ad(multiome_path)\n",
        "\n",
        "# Display basic information about the datasets\n",
        "print(\"CITE-seq Dataset:\")\n",
        "print(adata_cite)\n",
        "print(\"\\nMultiome Dataset:\")\n",
        "print(adata_multiome)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgP8dqYpnf63",
        "outputId": "25fab75e-d489-4532-f75a-3bb3f3cbbecd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
            "  utils.warn_names_duplicates(\"var\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CITE-seq Dataset:\n",
            "AnnData object with n_obs × n_vars = 90261 × 14087\n",
            "    obs: 'GEX_n_genes_by_counts', 'GEX_pct_counts_mt', 'GEX_size_factors', 'GEX_phase', 'ADT_n_antibodies_by_counts', 'ADT_total_counts', 'ADT_iso_count', 'cell_type', 'batch', 'ADT_pseudotime_order', 'GEX_pseudotime_order', 'Samplename', 'Site', 'DonorNumber', 'Modality', 'VendorLot', 'DonorID', 'DonorAge', 'DonorBMI', 'DonorBloodType', 'DonorRace', 'Ethnicity', 'DonorGender', 'QCMeds', 'DonorSmoker', 'is_train'\n",
            "    var: 'feature_types', 'gene_id'\n",
            "    uns: 'dataset_id', 'genome', 'organism'\n",
            "    obsm: 'ADT_X_pca', 'ADT_X_umap', 'ADT_isotype_controls', 'GEX_X_pca', 'GEX_X_umap'\n",
            "    layers: 'counts'\n",
            "\n",
            "Multiome Dataset:\n",
            "AnnData object with n_obs × n_vars = 69249 × 129921\n",
            "    obs: 'GEX_pct_counts_mt', 'GEX_n_counts', 'GEX_n_genes', 'GEX_size_factors', 'GEX_phase', 'ATAC_nCount_peaks', 'ATAC_atac_fragments', 'ATAC_reads_in_peaks_frac', 'ATAC_blacklist_fraction', 'ATAC_nucleosome_signal', 'cell_type', 'batch', 'ATAC_pseudotime_order', 'GEX_pseudotime_order', 'Samplename', 'Site', 'DonorNumber', 'Modality', 'VendorLot', 'DonorID', 'DonorAge', 'DonorBMI', 'DonorBloodType', 'DonorRace', 'Ethnicity', 'DonorGender', 'QCMeds', 'DonorSmoker'\n",
            "    var: 'feature_types', 'gene_id'\n",
            "    uns: 'ATAC_gene_activity_var_names', 'dataset_id', 'genome', 'organism'\n",
            "    obsm: 'ATAC_gene_activity', 'ATAC_lsi_full', 'ATAC_lsi_red', 'ATAC_umap', 'GEX_X_pca', 'GEX_X_umap'\n",
            "    layers: 'counts'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "y6pupaIydt32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class"
      ],
      "metadata": {
        "id": "PtJveqKI6Oqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import scipy.sparse\n",
        "import h5py\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, adata, modality=\"GEX\", use_gpu=False, chunk_size=5000, save_dir=\"/content/drive/MyDrive/ProjectX/\"):\n",
        "        \"\"\"\n",
        "        Initialize the dataset with sparse-to-dense conversion handled efficiently.\n",
        "        Args:\n",
        "            adata: AnnData object containing the data.\n",
        "            modality: The modality to extract (\"GEX\", \"ATAC\", \"ADT\").\n",
        "            use_gpu: Whether to use GPU for sparse-to-dense conversion.\n",
        "            chunk_size: Number of rows to process at a time if converting in chunks.\n",
        "            save_dir: Directory where dense matrices will be saved/loaded.\n",
        "        \"\"\"\n",
        "        self.adata = adata\n",
        "        self.modality = modality\n",
        "        self.use_gpu = use_gpu\n",
        "        self.chunk_size = chunk_size\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        # Define the file path for the dense matrix\n",
        "        self.data_path = os.path.join(self.save_dir, f\"{modality}_dense.h5\")\n",
        "        self.data = None  # Initialize self.data for direct use of dense data\n",
        "\n",
        "        # Check if a dense matrix file already exists\n",
        "        if os.path.exists(self.data_path):\n",
        "            print(f\"Dense matrix for {modality} found at {self.data_path}. Using saved file.\")\n",
        "        else:\n",
        "            # Check if the data is sparse or dense in-memory\n",
        "            if modality in self.adata.layers:\n",
        "                if scipy.sparse.issparse(self.adata.layers[modality]):\n",
        "                    print(f\"Converting sparse matrix for {modality} to dense.\")\n",
        "                    self.data_path = self._convert_sparse_to_dense_and_save(\n",
        "                        self.adata.layers[modality], self.data_path, modality\n",
        "                    )\n",
        "                else:\n",
        "                    print(f\"{modality} is already dense. Using in-memory data.\")\n",
        "                    self.data = self.adata.layers[modality]  # Use dense matrix directly\n",
        "            elif modality in self.adata.obsm:\n",
        "                if scipy.sparse.issparse(self.adata.obsm[modality]):\n",
        "                    print(f\"Converting sparse matrix for {modality} to dense.\")\n",
        "                    self.data_path = self._convert_sparse_to_dense_and_save(\n",
        "                        self.adata.obsm[modality], self.data_path, modality\n",
        "                    )\n",
        "                else:\n",
        "                    print(f\"{modality} is already dense. Using in-memory data.\")\n",
        "                    self.data = self.adata.obsm[modality]  # Use dense matrix directly\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported modality: {modality}\")\n",
        "\n",
        "        # Initialize metadata\n",
        "        self.metadata = self.adata.obs.reset_index(drop=True)\n",
        "\n",
        "        # Validate alignment of metadata with data if dense data is used directly\n",
        "        if self.data is not None:\n",
        "            assert self.data.shape[0] == self.metadata.shape[0], (\n",
        "                f\"Data and metadata mismatch: {self.data.shape[0]} rows in data, \"\n",
        "                f\"{self.metadata.shape[0]} rows in metadata.\"\n",
        "            )\n",
        "\n",
        "    def _convert_sparse_to_dense_and_save(self, sparse_matrix, output_file, dataset_name):\n",
        "        \"\"\"\n",
        "        Convert a sparse matrix to dense incrementally, save to HDF5, and return the file path.\n",
        "        \"\"\"\n",
        "        print(f\"Saving dense matrix for {dataset_name} to {output_file}\")\n",
        "        self.sparse_to_dense_in_chunks_with_save(sparse_matrix, output_file, dataset_name, self.chunk_size)\n",
        "        print(f\"Dense matrix saved to {output_file}.\")\n",
        "        return output_file  # Return the file path\n",
        "\n",
        "    @staticmethod\n",
        "    def sparse_to_dense_in_chunks_with_save(sparse_matrix, output_file, dataset_name, chunk_size):\n",
        "        \"\"\"\n",
        "        Convert a sparse matrix to dense in chunks and save it to an HDF5 file.\n",
        "        \"\"\"\n",
        "        n_rows, n_cols = sparse_matrix.shape\n",
        "        with h5py.File(output_file, \"w\") as f:\n",
        "            dset = f.create_dataset(dataset_name, shape=(n_rows, n_cols), dtype=np.float32)\n",
        "\n",
        "            for start in range(0, n_rows, chunk_size):\n",
        "                end = min(start + chunk_size, n_rows)\n",
        "                print(f\"Processing rows {start} to {end}\")\n",
        "                dense_chunk = sparse_matrix[start:end, :].toarray()\n",
        "                dset[start:end, :] = dense_chunk  # Write the chunk to disk\n",
        "                del dense_chunk  # Delete the dense chunk from memory\n",
        "                gc.collect()  # Trigger garbage collection\n",
        "            print(f\"Matrix saved to {output_file} as dataset '{dataset_name}'.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.metadata.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieve a single data point and its metadata.\n",
        "        \"\"\"\n",
        "        if idx >= len(self):\n",
        "            raise IndexError(f\"Index {idx} is out of range for dataset of size {len(self)}\")\n",
        "\n",
        "        # Access dense data from disk or memory\n",
        "        if self.data is not None:\n",
        "            data_sample = self.data[idx]  # Use in-memory dense data directly\n",
        "        else:\n",
        "            with h5py.File(self.data_path, \"r\") as f:\n",
        "                data_sample = f[self.modality][idx, :]  # Access data from disk\n",
        "\n",
        "        metadata_sample = self.metadata.iloc[idx]\n",
        "        return torch.tensor(data_sample, dtype=torch.float32), metadata_sample.to_dict()"
      ],
      "metadata": {
        "id": "iebcKe-TdMRg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GEX"
      ],
      "metadata": {
        "id": "rJjNNj-dydTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dataset for Multiome (GEX modality)\n",
        "print(\"\\nInitializing Multiome GEX dataset...\")\n",
        "try:\n",
        "    multiome_dataset_gex = MultiModalDataset(adata_multiome, modality=\"GEX\", use_gpu=False, chunk_size=10000)\n",
        "    multiome_gex_loader = DataLoader(multiome_dataset_gex, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Test Multiome GEX DataLoader\n",
        "    print(\"Multiome GEX DataLoader:\")\n",
        "    for batch_idx, (data, metadata) in enumerate(multiome_gex_loader):\n",
        "        print(f\"Batch {batch_idx}:\")\n",
        "        print(\"Data Shape:\", data.shape)\n",
        "        print(\"Metadata Sample:\", metadata)\n",
        "        break\n",
        "except Exception as e:\n",
        "    print(f\"Error in Multiome GEX DataLoader: {e}\")"
      ],
      "metadata": {
        "id": "VC4NSSCcyu82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d55bec79-bd29-4021-d85e-885d911150e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing Multiome GEX dataset...\n",
            "Dense matrix for GEX found at /content/drive/MyDrive/ProjectX/GEX_dense.h5. Using saved file.\n",
            "Multiome GEX DataLoader:\n",
            "Batch 0:\n",
            "Data Shape: torch.Size([32, 129921])\n",
            "Metadata Sample: {'GEX_pct_counts_mt': tensor([0.6700, 0.6360, 0.7414, 0.0270, 0.0000, 6.9333, 2.6016, 0.2933, 6.9307,\n",
            "        1.0758, 1.2270, 0.0000, 1.3774, 0.2865, 0.0000, 3.1325, 2.0243, 0.0000,\n",
            "        0.4197, 0.5698, 2.1448, 2.1138, 0.0000, 1.5291, 0.0426, 0.2862, 0.0000,\n",
            "        0.1031, 0.1543, 6.1538, 0.0000, 0.8147], dtype=torch.float64), 'GEX_n_counts': tensor([ 597., 1415., 1214., 3703., 1182.,  750.,  615.,  682.,  707., 2138.,\n",
            "        2771., 2050., 2904.,  698., 2837.,  830., 2470.,  915., 3336., 1755.,\n",
            "         373., 1230., 1832., 1962., 2350., 3145., 2460., 2911.,  648.,  390.,\n",
            "        1316., 2455.], dtype=torch.float64), 'GEX_n_genes': tensor([ 483,  798,  892, 2145,  849,  558,  476,  551,  514, 1190, 1428, 1291,\n",
            "        1577,  504, 1658,  603, 1599,  695, 1424, 1219,  321,  880, 1250, 1176,\n",
            "        1413, 1553, 1487, 1102,  511,  325,  991, 1300]), 'GEX_size_factors': tensor([0.4733, 0.2103, 0.3386, 2.2780, 0.2872, 0.5888, 0.3501, 0.1836, 0.5295,\n",
            "        0.6701, 0.8432, 0.8933, 0.8197, 0.1193, 1.2600, 0.2454, 1.6292, 0.4618,\n",
            "        0.8489, 0.6805, 0.1926, 0.8844, 0.5089, 0.5819, 0.6479, 0.8623, 0.9856,\n",
            "        0.5291, 0.1843, 0.3087, 0.3563, 0.7057], dtype=torch.float64), 'GEX_phase': ['G2M', 'G2M', 'G2M', 'S', 'G2M', 'G2M', 'G2M', 'S', 'G2M', 'G2M', 'G2M', 'G2M', 'S', 'G2M', 'G2M', 'S', 'G2M', 'G2M', 'S', 'G2M', 'S', 'G2M', 'S', 'G2M', 'S', 'G2M', 'S', 'G2M', 'G2M', 'G2M', 'G2M', 'G2M'], 'ATAC_nCount_peaks': tensor([ 3791.,  1676.,  2858., 15163.,  1312., 13791.,  6927.,  5860.,  4076.,\n",
            "         7295.,  6719.,  8675., 10831.,  1289., 11689.,  1628., 18496., 16037.,\n",
            "         5908., 10397.,  5475.,  4095.,  5252.,  5552.,  6145.,  3996., 11192.,\n",
            "         8900.,  6044.,  3300.,  3389.,  5037.], dtype=torch.float64), 'ATAC_atac_fragments': tensor([ 4164,  1546,  4917, 29121,  1709, 17753,  7656,  6569,  3575, 12207,\n",
            "         9250, 21294, 20194,   858, 30149,  1997, 23414, 53470,  8068, 16593,\n",
            "         6299,  4447,  6201,  9218,  4928,  3298, 30315,  6993,  4567,  2971,\n",
            "         3746,  8256]), 'ATAC_reads_in_peaks_frac': tensor([0.9104, 1.0841, 0.5812, 0.5207, 0.7677, 0.7768, 0.9048, 0.8921, 1.1401,\n",
            "        0.5976, 0.7264, 0.4074, 0.5363, 1.5023, 0.3877, 0.8152, 0.7900, 0.2999,\n",
            "        0.7323, 0.6266, 0.8692, 0.9208, 0.8470, 0.6023, 1.2470, 1.2116, 0.3692,\n",
            "        1.2727, 1.3234, 1.1107, 0.9047, 0.6101], dtype=torch.float64), 'ATAC_blacklist_fraction': tensor([0.0011, 0.0012, 0.0014, 0.0007, 0.0000, 0.0031, 0.0009, 0.0020, 0.0025,\n",
            "        0.0025, 0.0006, 0.0000, 0.0028, 0.0031, 0.0005, 0.0012, 0.0016, 0.0022,\n",
            "        0.0000, 0.0015, 0.0004, 0.0049, 0.0023, 0.0007, 0.0013, 0.0005, 0.0006,\n",
            "        0.0016, 0.0007, 0.0012, 0.0000, 0.0016], dtype=torch.float64), 'ATAC_nucleosome_signal': tensor([0.9215, 1.2701, 0.8604, 1.1274, 0.5366, 0.9225, 0.7073, 1.4926, 0.5839,\n",
            "        0.6097, 0.7117, 0.7970, 0.7133, 0.4188, 0.9175, 0.5983, 0.8393, 1.5870,\n",
            "        0.7858, 0.8645, 0.7864, 0.6905, 0.9847, 0.5850, 0.8048, 0.7862, 1.0685,\n",
            "        0.6950, 0.8199, 0.6760, 0.8515, 0.6215], dtype=torch.float64), 'cell_type': ['NK', 'Normoblast', 'G/M prog', 'G/M prog', 'NK', 'CD8+ T', 'CD8+ T', 'CD14+ Mono', 'CD8+ T', 'CD8+ T', 'CD4+ T naive', 'Lymph prog', 'CD14+ Mono', 'Normoblast', 'MK/E prog', 'ID2-hi myeloid prog', 'HSC', 'CD14+ Mono', 'CD4+ T naive', 'CD4+ T activated', 'NK', 'NK', 'NK', 'CD8+ T', 'CD14+ Mono', 'Erythroblast', 'pDC', 'Erythroblast', 'CD4+ T naive', 'CD8+ T', 'NK', 'CD4+ T naive'], 'batch': ['s4d1', 's2d4', 's2d1', 's3d3', 's3d6', 's4d9', 's4d8', 's2d4', 's4d9', 's1d3', 's1d2', 's3d10', 's1d3', 's2d4', 's3d10', 's1d1', 's4d8', 's3d3', 's1d2', 's1d2', 's4d8', 's4d8', 's2d5', 's1d3', 's2d4', 's2d4', 's3d10', 's2d4', 's2d4', 's4d9', 's2d5', 's1d3'], 'ATAC_pseudotime_order': tensor([   nan, 0.7759,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "           nan,    nan,    nan,    nan, 0.8778, 0.1713,    nan, 0.1494,    nan,\n",
            "           nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.7584,    nan,\n",
            "        0.8483,    nan,    nan,    nan,    nan], dtype=torch.float64), 'GEX_pseudotime_order': tensor([   nan, 0.9459,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "           nan,    nan,    nan,    nan, 0.9442, 0.0987,    nan, 0.0602,    nan,\n",
            "           nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.9363,    nan,\n",
            "        0.9336,    nan,    nan,    nan,    nan], dtype=torch.float64), 'Samplename': ['site4_donor1_multiome', 'site2_donor4_multiome', 'site2_donor1_multiome', 'site3_donor3_multiome', 'site3_donor6_multiome', 'site4_donor9_multiome', 'site4_donor8_multiome', 'site2_donor4_multiome', 'site4_donor9_multiome', 'site1_donor3_multiome', 'site1_donor2_multiome', 'site3_donor10_multiome', 'site1_donor3_multiome', 'site2_donor4_multiome', 'site3_donor10_multiome', 'site1_donor1_multiome', 'site4_donor8_multiome', 'site3_donor3_multiome', 'site1_donor2_multiome', 'site1_donor2_multiome', 'site4_donor8_multiome', 'site4_donor8_multiome', 'site2_donor5_multiome', 'site1_donor3_multiome', 'site2_donor4_multiome', 'site2_donor4_multiome', 'site3_donor10_multiome', 'site2_donor4_multiome', 'site2_donor4_multiome', 'site4_donor9_multiome', 'site2_donor5_multiome', 'site1_donor3_multiome'], 'Site': ['site4', 'site2', 'site2', 'site3', 'site3', 'site4', 'site4', 'site2', 'site4', 'site1', 'site1', 'site3', 'site1', 'site2', 'site3', 'site1', 'site4', 'site3', 'site1', 'site1', 'site4', 'site4', 'site2', 'site1', 'site2', 'site2', 'site3', 'site2', 'site2', 'site4', 'site2', 'site1'], 'DonorNumber': ['donor1', 'donor4', 'donor1', 'donor3', 'donor6', 'donor9', 'donor8', 'donor4', 'donor9', 'donor3', 'donor2', 'donor10', 'donor3', 'donor4', 'donor10', 'donor1', 'donor8', 'donor3', 'donor2', 'donor2', 'donor8', 'donor8', 'donor5', 'donor3', 'donor4', 'donor4', 'donor10', 'donor4', 'donor4', 'donor9', 'donor5', 'donor3'], 'Modality': ['multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome'], 'VendorLot': tensor([3054455, 3051683, 3054455, 3054716, 3059371, 3061608, 3051675, 3051683,\n",
            "        3061608, 3054716, 3054129, 3064467, 3054716, 3051683, 3064467, 3054455,\n",
            "        3051675, 3054716, 3054129, 3054129, 3051675, 3051675, 3054457, 3054716,\n",
            "        3051683, 3051683, 3064467, 3051683, 3051683, 3061608, 3054457, 3054716]), 'DonorID': tensor([15078, 12710, 15078, 18303, 28045, 13272, 19593, 12710, 13272, 18303,\n",
            "        10886, 28483, 18303, 12710, 28483, 15078, 19593, 18303, 10886, 10886,\n",
            "        19593, 19593, 16710, 18303, 12710, 12710, 28483, 12710, 12710, 13272,\n",
            "        16710, 18303]), 'DonorAge': tensor([34, 27, 34, 33, 36, 35, 31, 27, 35, 33, 35, 21, 33, 27, 21, 34, 31, 33,\n",
            "        35, 35, 31, 31, 40, 33, 27, 27, 21, 27, 27, 35, 40, 33]), 'DonorBMI': tensor([24.8000, 32.1000, 24.8000, 24.0000, 23.8000, 31.0000, 32.6000, 32.1000,\n",
            "        31.0000, 24.0000, 28.6000, 26.7000, 24.0000, 32.1000, 26.7000, 24.8000,\n",
            "        32.6000, 24.0000, 28.6000, 28.6000, 32.6000, 32.6000, 27.8000, 24.0000,\n",
            "        32.1000, 32.1000, 26.7000, 32.1000, 32.1000, 31.0000, 27.8000, 24.0000],\n",
            "       dtype=torch.float64), 'DonorBloodType': ['B-', 'O+', 'B-', 'O+', 'A+', 'O+', 'A+', 'O+', 'O+', 'O+', 'B+', 'O+', 'O+', 'O+', 'O+', 'B-', 'A+', 'O+', 'B+', 'B+', 'A+', 'A+', 'O+', 'O+', 'O+', 'O+', 'O+', 'O+', 'O+', 'O+', 'O+', 'O+'], 'DonorRace': ['White', 'White', 'White', 'Asian', 'Other Race', 'Other Race', 'Black or African American', 'White', 'Other Race', 'Asian', 'Asian', 'Other Race', 'Asian', 'White', 'Other Race', 'White', 'Black or African American', 'Asian', 'Asian', 'Asian', 'Black or African American', 'Black or African American', 'White', 'Asian', 'White', 'White', 'Other Race', 'White', 'White', 'Other Race', 'White', 'Asian'], 'Ethnicity': ['HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO'], 'DonorGender': ['Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male'], 'QCMeds': ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False'], 'DonorSmoker': ['Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Smoker', 'Smoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Smoker']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ATAC-seq"
      ],
      "metadata": {
        "id": "r3PX51gryfYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dataset for Multiome (ATAC modality)\n",
        "print(\"\\nInitializing Multiome ATAC dataset...\")\n",
        "try:\n",
        "    multiome_dataset_atac = MultiModalDataset(adata_multiome, modality=\"ATAC\", use_gpu=False, chunk_size=10000)\n",
        "    multiome_atac_loader = DataLoader(multiome_dataset_atac, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Test Multiome ATAC DataLoader\n",
        "    print(\"Multiome ATAC DataLoader:\")\n",
        "    for batch_idx, (data, metadata) in enumerate(multiome_atac_loader):\n",
        "        print(f\"Batch {batch_idx}:\")\n",
        "        print(\"Data Shape:\", data.shape)\n",
        "        print(\"Metadata Sample:\", metadata)\n",
        "        break\n",
        "except Exception as e:\n",
        "    print(f\"Error in Multiome ATAC DataLoader: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc3tt8egy6S5",
        "outputId": "6fba35ff-f5b1-41f6-c082-52fb70bc72c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing Multiome ATAC dataset...\n",
            "Dense matrix for ATAC found at /content/drive/MyDrive/ProjectX/ATAC_dense.h5. Using saved file.\n",
            "Multiome ATAC DataLoader:\n",
            "Batch 0:\n",
            "Data Shape: torch.Size([32, 129921])\n",
            "Metadata Sample: {'GEX_pct_counts_mt': tensor([1.1274, 0.0636, 0.6145, 0.2469, 7.1090, 1.9031, 2.0290, 0.0000, 1.3096,\n",
            "        0.0000, 0.0305, 1.5369, 0.9812, 0.4996, 0.2617, 0.0000, 1.5986, 1.0395,\n",
            "        0.9074, 0.9698, 1.2454, 0.3673, 0.2625, 0.8896, 0.0000, 0.2962, 2.4272,\n",
            "        2.0080, 1.7188, 0.0000, 0.0989, 2.6928], dtype=torch.float64), 'GEX_n_counts': tensor([1774., 1573., 1790., 3240.,  633.,  578.,  345., 3147., 1909.,  866.,\n",
            "        3281.,  976., 6115., 1401., 4203., 1463.,  563., 1924.,  551., 2681.,\n",
            "        1365., 1089.,  381., 4159.,  694., 1688., 1442., 1743., 2269.,  538.,\n",
            "        1011.,  817.], dtype=torch.float64), 'GEX_n_genes': tensor([1178, 1074, 1191, 1878,  488,  438,  298, 1717, 1192,  677, 1777,  699,\n",
            "        2683,  937, 2413, 1016,  372, 1115,  393, 1618,  952,  778,  317, 1422,\n",
            "         564, 1157,  948, 1177, 1399,  455,  768,  612]), 'GEX_size_factors': tensor([0.5780, 0.4220, 1.3808, 0.9483, 0.5610, 0.3998, 0.2466, 1.2244, 0.5525,\n",
            "        0.4511, 1.3303, 0.6927, 1.8783, 0.4047, 1.5708, 0.3924, 0.3482, 0.6188,\n",
            "        0.4030, 1.0086, 0.4112, 0.3360, 0.2311, 0.7679, 0.1972, 0.4603, 1.0078,\n",
            "        0.5640, 1.5274, 0.1519, 0.5192, 0.5711], dtype=torch.float64), 'GEX_phase': ['S', 'G2M', 'G2M', 'G2M', 'G2M', 'G2M', 'S', 'S', 'G2M', 'S', 'G2M', 'G2M', 'G2M', 'G2M', 'S', 'G2M', 'G2M', 'S', 'G2M', 'G2M', 'S', 'S', 'G2M', 'S', 'G2M', 'S', 'G2M', 'G2M', 'G2M', 'S', 'S', 'G2M'], 'ATAC_nCount_peaks': tensor([ 9834.,  1665., 28923.,   938., 11066.,  2683., 19537.,  7875.,  1591.,\n",
            "        31367.,  9462.,  5413., 11148.,  8617.,   298.,   427.,  8707.,  2319.,\n",
            "         2438.,  8826.,  6058.,  6513.,  6012.,  9058.,   549.,   505.,  7765.,\n",
            "         7044., 10687.,  4793., 28282.,  8055.], dtype=torch.float64), 'ATAC_atac_fragments': tensor([20198,  2470, 40097,  1256,  9953,  2888, 21770, 17803,  2032, 66762,\n",
            "        25646,  5675, 20246,  7880,   423,   452, 11671,  2893,  2777, 12582,\n",
            "         9665,  5462,  6578, 19815,  1118,   538,  9130, 12795, 11740, 61537,\n",
            "        54433,  8978]), 'ATAC_reads_in_peaks_frac': tensor([0.4869, 0.6741, 0.7213, 0.7468, 1.1118, 0.9290, 0.8974, 0.4423, 0.7830,\n",
            "        0.4698, 0.3689, 0.9538, 0.5506, 1.0935, 0.7045, 0.9447, 0.7460, 0.8016,\n",
            "        0.8779, 0.7015, 0.6268, 1.1924, 0.9140, 0.4571, 0.4911, 0.9387, 0.8505,\n",
            "        0.5505, 0.9103, 0.0779, 0.5196, 0.8972], dtype=torch.float64), 'ATAC_blacklist_fraction': tensor([0.0012, 0.0012, 0.0022, 0.0043, 0.0021, 0.0007, 0.0012, 0.0008, 0.0038,\n",
            "        0.0008, 0.0002, 0.0026, 0.0019, 0.0014, 0.0000, 0.0000, 0.0016, 0.0000,\n",
            "        0.0016, 0.0036, 0.0007, 0.0006, 0.0040, 0.0022, 0.0000, 0.0000, 0.0010,\n",
            "        0.0014, 0.0022, 0.0004, 0.0007, 0.0010], dtype=torch.float64), 'ATAC_nucleosome_signal': tensor([0.6378, 1.0131, 0.8503, 0.8571, 0.7121, 0.7896, 0.8194, 0.9743, 1.1049,\n",
            "        1.3571, 1.0857, 0.6051, 0.8552, 1.0268, 1.0000, 0.6607, 1.2826, 0.6078,\n",
            "        0.6979, 0.8093, 1.0364, 0.8796, 0.7694, 0.9066, 1.0333, 0.7429, 0.8297,\n",
            "        0.8580, 0.8815, 1.8362, 1.2953, 0.8576], dtype=torch.float64), 'cell_type': ['CD4+ T naive', 'CD4+ T activated', 'pDC', 'Lymph prog', 'CD8+ T naive', 'CD8+ T', 'CD4+ T activated', 'CD14+ Mono', 'Naive CD20+ B', 'Proerythroblast', 'pDC', 'Naive CD20+ B', 'CD4+ T naive', 'Naive CD20+ B', 'Proerythroblast', 'CD4+ T activated', 'Erythroblast', 'CD4+ T naive', 'Transitional B', 'B1 B', 'CD14+ Mono', 'CD4+ T naive', 'CD4+ T naive', 'Erythroblast', 'CD14+ Mono', 'CD4+ T activated', 'CD4+ T activated', 'NK', 'CD8+ T', 'CD14+ Mono', 'B1 B', 'CD8+ T'], 'batch': ['s1d3', 's3d6', 's4d1', 's2d1', 's4d9', 's4d8', 's4d8', 's3d10', 's2d1', 's3d3', 's3d10', 's4d8', 's1d3', 's2d4', 's2d4', 's2d5', 's4d1', 's1d2', 's4d1', 's1d2', 's1d1', 's2d4', 's4d8', 's1d3', 's3d6', 's2d5', 's4d8', 's1d3', 's4d8', 's3d7', 's3d3', 's4d8'], 'ATAC_pseudotime_order': tensor([   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "        0.7224,    nan,    nan,    nan,    nan, 0.6757,    nan, 0.7823,    nan,\n",
            "           nan,    nan,    nan,    nan,    nan, 0.8069,    nan,    nan,    nan,\n",
            "           nan,    nan,    nan,    nan,    nan], dtype=torch.float64), 'GEX_pseudotime_order': tensor([   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "        0.5224,    nan,    nan,    nan,    nan, 0.7461,    nan, 0.7098,    nan,\n",
            "           nan,    nan,    nan,    nan,    nan, 0.9627,    nan,    nan,    nan,\n",
            "           nan,    nan,    nan,    nan,    nan], dtype=torch.float64), 'Samplename': ['site1_donor3_multiome', 'site3_donor6_multiome', 'site4_donor1_multiome', 'site2_donor1_multiome', 'site4_donor9_multiome', 'site4_donor8_multiome', 'site4_donor8_multiome', 'site3_donor10_multiome', 'site2_donor1_multiome', 'site3_donor3_multiome', 'site3_donor10_multiome', 'site4_donor8_multiome', 'site1_donor3_multiome', 'site2_donor4_multiome', 'site2_donor4_multiome', 'site2_donor5_multiome', 'site4_donor1_multiome', 'site1_donor2_multiome', 'site4_donor1_multiome', 'site1_donor2_multiome', 'site1_donor1_multiome', 'site2_donor4_multiome', 'site4_donor8_multiome', 'site1_donor3_multiome', 'site3_donor6_multiome', 'site2_donor5_multiome', 'site4_donor8_multiome', 'site1_donor3_multiome', 'site4_donor8_multiome', 'site3_donor7_multiome', 'site3_donor3_multiome', 'site4_donor8_multiome'], 'Site': ['site1', 'site3', 'site4', 'site2', 'site4', 'site4', 'site4', 'site3', 'site2', 'site3', 'site3', 'site4', 'site1', 'site2', 'site2', 'site2', 'site4', 'site1', 'site4', 'site1', 'site1', 'site2', 'site4', 'site1', 'site3', 'site2', 'site4', 'site1', 'site4', 'site3', 'site3', 'site4'], 'DonorNumber': ['donor3', 'donor6', 'donor1', 'donor1', 'donor9', 'donor8', 'donor8', 'donor10', 'donor1', 'donor3', 'donor10', 'donor8', 'donor3', 'donor4', 'donor4', 'donor5', 'donor1', 'donor2', 'donor1', 'donor2', 'donor1', 'donor4', 'donor8', 'donor3', 'donor6', 'donor5', 'donor8', 'donor3', 'donor8', 'donor7', 'donor3', 'donor8'], 'Modality': ['multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome'], 'VendorLot': tensor([3054716, 3059371, 3054455, 3054455, 3061608, 3051675, 3051675, 3064467,\n",
            "        3054455, 3054716, 3064467, 3051675, 3054716, 3051683, 3051683, 3054457,\n",
            "        3054455, 3054129, 3054455, 3054129, 3054455, 3051683, 3051675, 3054716,\n",
            "        3059371, 3054457, 3051675, 3054716, 3051675, 3054734, 3054716, 3051675]), 'DonorID': tensor([18303, 28045, 15078, 15078, 13272, 19593, 19593, 28483, 15078, 18303,\n",
            "        28483, 19593, 18303, 12710, 12710, 16710, 15078, 10886, 15078, 10886,\n",
            "        15078, 12710, 19593, 18303, 28045, 16710, 19593, 18303, 19593, 11466,\n",
            "        18303, 19593]), 'DonorAge': tensor([33, 36, 34, 34, 35, 31, 31, 21, 34, 33, 21, 31, 33, 27, 27, 40, 34, 35,\n",
            "        34, 35, 34, 27, 31, 33, 36, 40, 31, 33, 31, 22, 33, 31]), 'DonorBMI': tensor([24.0000, 23.8000, 24.8000, 24.8000, 31.0000, 32.6000, 32.6000, 26.7000,\n",
            "        24.8000, 24.0000, 26.7000, 32.6000, 24.0000, 32.1000, 32.1000, 27.8000,\n",
            "        24.8000, 28.6000, 24.8000, 28.6000, 24.8000, 32.1000, 32.6000, 24.0000,\n",
            "        23.8000, 27.8000, 32.6000, 24.0000, 32.6000, 31.5000, 24.0000, 32.6000],\n",
            "       dtype=torch.float64), 'DonorBloodType': ['O+', 'A+', 'B-', 'B-', 'O+', 'A+', 'A+', 'O+', 'B-', 'O+', 'O+', 'A+', 'O+', 'O+', 'O+', 'O+', 'B-', 'B+', 'B-', 'B+', 'B-', 'O+', 'A+', 'O+', 'A+', 'O+', 'A+', 'O+', 'A+', 'A+', 'O+', 'A+'], 'DonorRace': ['Asian', 'Other Race', 'White', 'White', 'Other Race', 'Black or African American', 'Black or African American', 'Other Race', 'White', 'Asian', 'Other Race', 'Black or African American', 'Asian', 'White', 'White', 'White', 'White', 'Asian', 'White', 'Asian', 'White', 'White', 'Black or African American', 'Asian', 'Other Race', 'White', 'Black or African American', 'Asian', 'Black or African American', 'Asian', 'Asian', 'Black or African American'], 'Ethnicity': ['NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO'], 'DonorGender': ['Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male'], 'QCMeds': ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False'], 'DonorSmoker': ['Smoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Smoker', 'Smoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Nonsmoker']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CITE-seq"
      ],
      "metadata": {
        "id": "XG9lS79myY3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Available layers:\", adata_cite.layers.keys())\n",
        "print(\"Available obsm keys:\", adata_cite.obsm.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tVJj03iOzFj",
        "outputId": "5fa3004e-deda-4d5a-d2a9-9c9d51b68028"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available layers: KeysView(Layers with keys: counts)\n",
            "Available obsm keys: KeysView(AxisArrays with keys: ADT_X_pca, ADT_X_umap, ADT_isotype_controls, GEX_X_pca, GEX_X_umap)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CITE-seq dataloader is functional for every modality except \"ADT_isotype_controls\"**"
      ],
      "metadata": {
        "id": "vt9GuzgkQYmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the ADT modality is correctly identified in MultiModalDataset\n",
        "cite_dataset = MultiModalDataset(adata_cite, modality=\"ADT_X_umap\", use_gpu=False, chunk_size=10000)\n",
        "\n",
        "# Debug CITE-seq DataLoader\n",
        "cite_loader = DataLoader(cite_dataset, batch_size=32, shuffle=True)\n",
        "for idx, (data, metadata) in enumerate(cite_loader):\n",
        "    print(f\"Batch {idx}: Data shape {data.shape}, Metadata shape {len(metadata)}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-imUn0V7CCM",
        "outputId": "b5c6f739-0d09-49cc-c73a-03839227c23c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADT_X_umap is already dense. Using in-memory data.\n",
            "Batch 0: Data shape torch.Size([32, 2]), Metadata shape 26\n"
          ]
        }
      ]
    }
  ]
}