{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PPTm2zpddUTU",
        "mdHOCmRldZeR",
        "PtJveqKI6Oqc",
        "XG9lS79myY3V",
        "r3PX51gryfYD"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download & Uncompress Datasets"
      ],
      "metadata": {
        "id": "PPTm2zpddUTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# import requests\n",
        "# import gzip\n",
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# # Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Define URLs and file paths\n",
        "# urls = {\n",
        "#     \"CITE-seq\": \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE194122&format=file&file=GSE194122%5Fopenproblems%5Fneurips2021%5Fcite%5FBMMC%5Fprocessed%2Eh5ad%2Egz\",\n",
        "#     \"Multiome\": \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE194122&format=file&file=GSE194122%5Fopenproblems%5Fneurips2021%5Fmultiome%5FBMMC%5Fprocessed%2Eh5ad%2Egz\"\n",
        "# }\n",
        "\n",
        "# output_dir = \"/content/drive/MyDrive/ProjectX/data\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# for name, url in urls.items():\n",
        "#     output_gz = os.path.join(output_dir, f\"{name}.h5ad.gz\")\n",
        "#     output_file = os.path.join(output_dir, f\"{name}.h5ad\")\n",
        "\n",
        "#     # Download the dataset\n",
        "#     print(f\"Downloading {name} dataset...\")\n",
        "#     response = requests.get(url, stream=True)\n",
        "#     with open(output_gz, 'wb') as f:\n",
        "#         for chunk in response.iter_content(chunk_size=1024):\n",
        "#             if chunk:\n",
        "#                 f.write(chunk)\n",
        "#     print(f\"{name} dataset saved to {output_gz}\")\n",
        "\n",
        "#     # Decompress the file\n",
        "#     print(f\"Decompressing {name} dataset...\")\n",
        "#     with gzip.open(output_gz, 'rb') as f_in:\n",
        "#         with open(output_file, 'wb') as f_out:\n",
        "#             shutil.copyfileobj(f_in, f_out)\n",
        "#     print(f\"{name} dataset decompressed and saved to {output_file}\")\n",
        "\n",
        "#     # Optional: Delete the .gz file to save space\n",
        "#     os.remove(output_gz)\n",
        "#     print(f\"Compressed file {output_gz} deleted.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a72DPzJaZl_m",
        "outputId": "b4f53b59-74ab-41bc-92f9-d674004ae990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Downloading CITE-seq dataset...\n",
            "CITE-seq dataset saved to /content/drive/MyDrive/ProjectX/data/CITE-seq.h5ad.gz\n",
            "Decompressing CITE-seq dataset...\n",
            "CITE-seq dataset decompressed and saved to /content/drive/MyDrive/ProjectX/data/CITE-seq.h5ad\n",
            "Compressed file /content/drive/MyDrive/ProjectX/data/CITE-seq.h5ad.gz deleted.\n",
            "\n",
            "Downloading Multiome dataset...\n",
            "Multiome dataset saved to /content/drive/MyDrive/ProjectX/data/Multiome.h5ad.gz\n",
            "Decompressing Multiome dataset...\n",
            "Multiome dataset decompressed and saved to /content/drive/MyDrive/ProjectX/data/Multiome.h5ad\n",
            "Compressed file /content/drive/MyDrive/ProjectX/data/Multiome.h5ad.gz deleted.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs & Loading Datasets"
      ],
      "metadata": {
        "id": "mdHOCmRldZeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8EmnrOpY_fR",
        "outputId": "404b9bf9-d2c9-4898-e726-9d312ff888cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scanpy torch numpy pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug98nBdvZTGe",
        "outputId": "548593b8-74fe-4373-c686-5f8ccc0d02cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scanpy\n",
            "  Downloading scanpy-1.10.4-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Collecting anndata>=0.8 (from scanpy)\n",
            "  Downloading anndata-0.11.1-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: h5py>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.12.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.4.2)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.8.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.4.2)\n",
            "Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (24.2)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.0.1)\n",
            "Collecting pynndescent>=0.5 (from scanpy)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.1 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.13.2)\n",
            "Collecting session-info (from scanpy)\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy) (4.66.6)\n",
            "Collecting umap-learn!=0.5.0,>=0.5 (from scanpy)\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Collecting array-api-compat!=1.5,>1.4 (from anndata>=0.8->scanpy)\n",
            "  Downloading array_api_compat-1.9.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.56->scanpy) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1->scanpy) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting stdlib_list (from session-info->scanpy)\n",
            "  Downloading stdlib_list-0.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Downloading scanpy-1.10.4-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anndata-0.11.1-py3-none-any.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4-py3-none-any.whl (15 kB)\n",
            "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading array_api_compat-1.9.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stdlib_list-0.11.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: session-info\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8023 sha256=c40e34283ec36a92fb49223c8873e453eee85155e85e276becd10dc08552babc\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
            "Successfully built session-info\n",
            "Installing collected packages: stdlib_list, legacy-api-wrap, array-api-compat, session-info, pynndescent, anndata, umap-learn, scanpy\n",
            "Successfully installed anndata-0.11.1 array-api-compat-1.9.1 legacy-api-wrap-1.4 pynndescent-0.5.13 scanpy-1.10.4 session-info-1.0.0 stdlib_list-0.11.0 umap-learn-0.5.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scanpy as sc\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define the paths to the datasets\n",
        "cite_path = \"/content/drive/MyDrive/ProjectX/data/CITE-seq.h5ad\"\n",
        "multiome_path = \"/content/drive/MyDrive/ProjectX/data/Multiome.h5ad\"\n",
        "\n",
        "# Load the datasets using scanpy\n",
        "adata_cite = sc.read_h5ad(cite_path)\n",
        "adata_multiome = sc.read_h5ad(multiome_path)\n",
        "\n",
        "# Display basic information about the datasets\n",
        "print(\"CITE-seq Dataset:\")\n",
        "print(adata_cite)\n",
        "print(\"\\nMultiome Dataset:\")\n",
        "print(adata_multiome)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgP8dqYpnf63",
        "outputId": "da09d02c-9c0e-4bce-85d3-0db202183fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/anndata/_core/anndata.py:1758: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
            "  utils.warn_names_duplicates(\"var\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CITE-seq Dataset:\n",
            "AnnData object with n_obs × n_vars = 90261 × 14087\n",
            "    obs: 'GEX_n_genes_by_counts', 'GEX_pct_counts_mt', 'GEX_size_factors', 'GEX_phase', 'ADT_n_antibodies_by_counts', 'ADT_total_counts', 'ADT_iso_count', 'cell_type', 'batch', 'ADT_pseudotime_order', 'GEX_pseudotime_order', 'Samplename', 'Site', 'DonorNumber', 'Modality', 'VendorLot', 'DonorID', 'DonorAge', 'DonorBMI', 'DonorBloodType', 'DonorRace', 'Ethnicity', 'DonorGender', 'QCMeds', 'DonorSmoker', 'is_train'\n",
            "    var: 'feature_types', 'gene_id'\n",
            "    uns: 'dataset_id', 'genome', 'organism'\n",
            "    obsm: 'ADT_X_pca', 'ADT_X_umap', 'ADT_isotype_controls', 'GEX_X_pca', 'GEX_X_umap'\n",
            "    layers: 'counts'\n",
            "\n",
            "Multiome Dataset:\n",
            "AnnData object with n_obs × n_vars = 69249 × 129921\n",
            "    obs: 'GEX_pct_counts_mt', 'GEX_n_counts', 'GEX_n_genes', 'GEX_size_factors', 'GEX_phase', 'ATAC_nCount_peaks', 'ATAC_atac_fragments', 'ATAC_reads_in_peaks_frac', 'ATAC_blacklist_fraction', 'ATAC_nucleosome_signal', 'cell_type', 'batch', 'ATAC_pseudotime_order', 'GEX_pseudotime_order', 'Samplename', 'Site', 'DonorNumber', 'Modality', 'VendorLot', 'DonorID', 'DonorAge', 'DonorBMI', 'DonorBloodType', 'DonorRace', 'Ethnicity', 'DonorGender', 'QCMeds', 'DonorSmoker'\n",
            "    var: 'feature_types', 'gene_id'\n",
            "    uns: 'ATAC_gene_activity_var_names', 'dataset_id', 'genome', 'organism'\n",
            "    obsm: 'ATAC_gene_activity', 'ATAC_lsi_full', 'ATAC_lsi_red', 'ATAC_umap', 'GEX_X_pca', 'GEX_X_umap'\n",
            "    layers: 'counts'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scanpy as sc\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_data(adata, modality):\n",
        "    \"\"\"\n",
        "    Preprocess the AnnData object based on the modality.\n",
        "    Args:\n",
        "        adata: AnnData object.\n",
        "        modality: The modality to preprocess (\"GEX\", \"ATAC\", \"ADT\").\n",
        "    \"\"\"\n",
        "    if modality == \"GEX\":\n",
        "        # RNA modality: Normalize and log-transform RNA data\n",
        "        adata.layers[\"GEX\"] = sc.pp.log1p(adata.layers[\"counts\"], copy=True)\n",
        "    elif modality == \"ATAC\":\n",
        "        # DNA modality: Binarize ATAC data (peaks presence/absence)\n",
        "        adata.layers[\"ATAC\"] = (adata.layers[\"counts\"] > 0).astype(int)\n",
        "    elif modality == \"ADT\":\n",
        "        # Protein modality: CLR normalization for ADT data\n",
        "        if \"ADT_isotype_controls\" in adata.obsm:\n",
        "            adata.obsm[\"ADT\"] = (\n",
        "                adata.obsm[\"ADT_isotype_controls\"] -\n",
        "                adata.obsm[\"ADT_isotype_controls\"].mean(axis=0)\n",
        "            ) / adata.obsm[\"ADT_isotype_controls\"].std(axis=0)\n",
        "        else:\n",
        "            raise KeyError(\"ADT data not found in obsm.\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported modality: {modality}\")\n",
        "\n",
        "    return adata\n",
        "\n",
        "# Step 1: Reset indices to ensure alignment between data and metadata\n",
        "adata_cite.obs = adata_cite.obs.reset_index(drop=True)\n",
        "adata_multiome.obs = adata_multiome.obs.reset_index(drop=True)\n",
        "\n",
        "# Step 2: Ensure unique variable names in both datasets\n",
        "adata_cite.var_names_make_unique()\n",
        "adata_multiome.var_names_make_unique()\n",
        "\n",
        "# Step 3: Check all modalities in the CITE-seq dataset\n",
        "print(\"Inspecting CITE-seq dataset...\")\n",
        "for layer in adata_cite.layers.keys():\n",
        "    print(f\"Layer '{layer}' Shape:\", adata_cite.layers[layer].shape)\n",
        "\n",
        "for obsm_key in adata_cite.obsm.keys():\n",
        "    print(f\"Obsm Key '{obsm_key}' Shape:\", adata_cite.obsm[obsm_key].shape)\n",
        "\n",
        "# Step 4: Assert consistency between counts data and metadata for CITE-seq\n",
        "assert adata_cite.layers[\"counts\"].shape[0] == adata_cite.obs.shape[0], (\n",
        "    \"Mismatch in CITE-seq data and metadata dimensions!\"\n",
        ")\n",
        "\n",
        "# Step 5: Check all modalities in the Multiome dataset\n",
        "print(\"\\nInspecting Multiome dataset...\")\n",
        "for layer in adata_multiome.layers.keys():\n",
        "    print(f\"Layer '{layer}' Shape:\", adata_multiome.layers[layer].shape)\n",
        "\n",
        "for obsm_key in adata_multiome.obsm.keys():\n",
        "    print(f\"Obsm Key '{obsm_key}' Shape:\", adata_multiome.obsm[obsm_key].shape)\n",
        "\n",
        "# Step 6: Assert consistency between counts data and metadata for Multiome\n",
        "assert adata_multiome.layers[\"counts\"].shape[0] == adata_multiome.obs.shape[0], (\n",
        "    \"Mismatch in Multiome data and metadata dimensions!\"\n",
        ")\n",
        "\n",
        "# Step 7: Debug ADT processing specifically for the CITE-seq dataset\n",
        "if \"ADT_isotype_controls\" in adata_cite.obsm:\n",
        "    print(\"\\nADT_isotype_controls found. Processing ADT data...\")\n",
        "    adata_cite.obsm[\"ADT\"] = (\n",
        "        adata_cite.obsm[\"ADT_isotype_controls\"] -\n",
        "        adata_cite.obsm[\"ADT_isotype_controls\"].mean(axis=0)\n",
        "    ) / adata_cite.obsm[\"ADT_isotype_controls\"].std(axis=0)\n",
        "    print(f\"Processed ADT data shape: {adata_cite.obsm['ADT'].shape}\")\n",
        "else:\n",
        "    raise KeyError(\"ADT data not found in obsm. Ensure the dataset includes ADT information.\")\n",
        "\n",
        "# Step 8: Inspect dataset dimensions\n",
        "print(\"\\nFinal dataset checks:\")\n",
        "print(\"CITE-seq Data Matrix Shape:\", adata_cite.layers[\"counts\"].shape)\n",
        "print(\"CITE-seq Metadata Shape:\", adata_cite.obs.shape)\n",
        "print(\"Multiome Data Matrix Shape:\", adata_multiome.layers[\"counts\"].shape)\n",
        "print(\"Multiome Metadata Shape:\", adata_multiome.obs.shape)\n",
        "\n",
        "# Step 9: Preprocess each dataset based on its modality\n",
        "print(\"\\nPreprocessing datasets based on modalities...\")\n",
        "adata_cite = preprocess_data(adata_cite, \"ADT\")  # Protein data for CITE-seq\n",
        "adata_multiome = preprocess_data(adata_multiome, \"GEX\")  # RNA data for Multiome\n",
        "adata_multiome = preprocess_data(adata_multiome, \"ATAC\")  # DNA data for Multiome\n",
        "\n",
        "print(\"\\nPreprocessing complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClH-6NIpnp8Q",
        "outputId": "e28d41d6-7731-43bf-9a23-d2c29594b340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inspecting CITE-seq dataset...\n",
            "Layer 'counts' Shape: (90261, 14087)\n",
            "Obsm Key 'ADT_X_pca' Shape: (90261, 50)\n",
            "Obsm Key 'ADT_X_umap' Shape: (90261, 2)\n",
            "Obsm Key 'ADT_isotype_controls' Shape: (90261, 6)\n",
            "Obsm Key 'GEX_X_pca' Shape: (90261, 50)\n",
            "Obsm Key 'GEX_X_umap' Shape: (90261, 2)\n",
            "Obsm Key 'ADT' Shape: (90261, 6)\n",
            "\n",
            "Inspecting Multiome dataset...\n",
            "Layer 'counts' Shape: (69249, 129921)\n",
            "Layer 'GEX' Shape: (69249, 129921)\n",
            "Layer 'ATAC' Shape: (69249, 129921)\n",
            "Obsm Key 'ATAC_gene_activity' Shape: (69249, 19039)\n",
            "Obsm Key 'ATAC_lsi_full' Shape: (69249, 50)\n",
            "Obsm Key 'ATAC_lsi_red' Shape: (69249, 39)\n",
            "Obsm Key 'ATAC_umap' Shape: (69249, 2)\n",
            "Obsm Key 'GEX_X_pca' Shape: (69249, 50)\n",
            "Obsm Key 'GEX_X_umap' Shape: (69249, 2)\n",
            "\n",
            "ADT_isotype_controls found. Processing ADT data...\n",
            "Processed ADT data shape: (90261, 6)\n",
            "\n",
            "Final dataset checks:\n",
            "CITE-seq Data Matrix Shape: (90261, 14087)\n",
            "CITE-seq Metadata Shape: (90261, 26)\n",
            "Multiome Data Matrix Shape: (69249, 129921)\n",
            "Multiome Metadata Shape: (69249, 28)\n",
            "\n",
            "Preprocessing datasets based on modalities...\n",
            "\n",
            "Preprocessing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "y6pupaIydt32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class"
      ],
      "metadata": {
        "id": "PtJveqKI6Oqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import scipy.sparse\n",
        "import h5py\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, adata, modality=\"GEX\", use_gpu=False, chunk_size=5000, save_dir=\"/content/drive/MyDrive/ProjectX/\"):\n",
        "        \"\"\"\n",
        "        Initialize the dataset with sparse-to-dense conversion handled efficiently.\n",
        "        Args:\n",
        "            adata: AnnData object containing the data.\n",
        "            modality: The modality to extract (\"GEX\", \"ATAC\", \"ADT\").\n",
        "            use_gpu: Whether to use GPU for sparse-to-dense conversion.\n",
        "            chunk_size: Number of rows to process at a time if converting in chunks.\n",
        "            save_dir: Directory where dense matrices will be saved/loaded.\n",
        "        \"\"\"\n",
        "        self.adata = adata\n",
        "        self.modality = modality\n",
        "        self.use_gpu = use_gpu\n",
        "        self.chunk_size = chunk_size\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        # Define the file path for the dense matrix\n",
        "        self.data_path = os.path.join(self.save_dir, f\"{modality}_dense.h5\")\n",
        "        self.data = None  # Initialize self.data for direct use of dense data\n",
        "\n",
        "        # Check if a dense matrix file already exists\n",
        "        if os.path.exists(self.data_path):\n",
        "            print(f\"Dense matrix for {modality} found at {self.data_path}. Using saved file.\")\n",
        "        else:\n",
        "            # Check if the data is sparse or dense in-memory\n",
        "            if modality in self.adata.layers:\n",
        "                if scipy.sparse.issparse(self.adata.layers[modality]):\n",
        "                    print(f\"Converting sparse matrix for {modality} to dense.\")\n",
        "                    self.data_path = self._convert_sparse_to_dense_and_save(\n",
        "                        self.adata.layers[modality], self.data_path, modality\n",
        "                    )\n",
        "                else:\n",
        "                    print(f\"{modality} is already dense. Using in-memory data.\")\n",
        "                    self.data = self.adata.layers[modality]  # Use dense matrix directly\n",
        "            elif modality in self.adata.obsm:\n",
        "                if scipy.sparse.issparse(self.adata.obsm[modality]):\n",
        "                    print(f\"Converting sparse matrix for {modality} to dense.\")\n",
        "                    self.data_path = self._convert_sparse_to_dense_and_save(\n",
        "                        self.adata.obsm[modality], self.data_path, modality\n",
        "                    )\n",
        "                else:\n",
        "                    print(f\"{modality} is already dense. Using in-memory data.\")\n",
        "                    self.data = self.adata.obsm[modality]  # Use dense matrix directly\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported modality: {modality}\")\n",
        "\n",
        "        # Initialize metadata\n",
        "        self.metadata = self.adata.obs.reset_index(drop=True)\n",
        "\n",
        "        # Validate alignment of metadata with data if dense data is used directly\n",
        "        if self.data is not None:\n",
        "            assert self.data.shape[0] == self.metadata.shape[0], (\n",
        "                f\"Data and metadata mismatch: {self.data.shape[0]} rows in data, \"\n",
        "                f\"{self.metadata.shape[0]} rows in metadata.\"\n",
        "            )\n",
        "\n",
        "    def _convert_sparse_to_dense_and_save(self, sparse_matrix, output_file, dataset_name):\n",
        "        \"\"\"\n",
        "        Convert a sparse matrix to dense incrementally, save to HDF5, and return the file path.\n",
        "        \"\"\"\n",
        "        print(f\"Saving dense matrix for {dataset_name} to {output_file}\")\n",
        "        self.sparse_to_dense_in_chunks_with_save(sparse_matrix, output_file, dataset_name, self.chunk_size)\n",
        "        print(f\"Dense matrix saved to {output_file}.\")\n",
        "        return output_file  # Return the file path\n",
        "\n",
        "    @staticmethod\n",
        "    def sparse_to_dense_in_chunks_with_save(sparse_matrix, output_file, dataset_name, chunk_size):\n",
        "        \"\"\"\n",
        "        Convert a sparse matrix to dense in chunks and save it to an HDF5 file.\n",
        "        \"\"\"\n",
        "        n_rows, n_cols = sparse_matrix.shape\n",
        "        with h5py.File(output_file, \"w\") as f:\n",
        "            dset = f.create_dataset(dataset_name, shape=(n_rows, n_cols), dtype=np.float32)\n",
        "\n",
        "            for start in range(0, n_rows, chunk_size):\n",
        "                end = min(start + chunk_size, n_rows)\n",
        "                print(f\"Processing rows {start} to {end}\")\n",
        "                dense_chunk = sparse_matrix[start:end, :].toarray()\n",
        "                dset[start:end, :] = dense_chunk  # Write the chunk to disk\n",
        "                del dense_chunk  # Delete the dense chunk from memory\n",
        "                gc.collect()  # Trigger garbage collection\n",
        "            print(f\"Matrix saved to {output_file} as dataset '{dataset_name}'.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.metadata.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieve a single data point and its metadata.\n",
        "        \"\"\"\n",
        "        if idx >= len(self):\n",
        "            raise IndexError(f\"Index {idx} is out of range for dataset of size {len(self)}\")\n",
        "\n",
        "        # Access dense data from disk or memory\n",
        "        if self.data is not None:\n",
        "            data_sample = self.data[idx]  # Use in-memory dense data directly\n",
        "        else:\n",
        "            with h5py.File(self.data_path, \"r\") as f:\n",
        "                data_sample = f[self.modality][idx, :]  # Access data from disk\n",
        "\n",
        "        metadata_sample = self.metadata.iloc[idx]\n",
        "        return torch.tensor(data_sample, dtype=torch.float32), metadata_sample.to_dict()"
      ],
      "metadata": {
        "id": "iebcKe-TdMRg"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GEX"
      ],
      "metadata": {
        "id": "rJjNNj-dydTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dataset for Multiome (GEX modality)\n",
        "print(\"\\nInitializing Multiome GEX dataset...\")\n",
        "try:\n",
        "    multiome_dataset_gex = MultiModalDataset(adata_multiome, modality=\"GEX\", use_gpu=False, chunk_size=10000)\n",
        "    multiome_gex_loader = DataLoader(multiome_dataset_gex, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Test Multiome GEX DataLoader\n",
        "    print(\"Multiome GEX DataLoader:\")\n",
        "    for batch_idx, (data, metadata) in enumerate(multiome_gex_loader):\n",
        "        print(f\"Batch {batch_idx}:\")\n",
        "        print(\"Data Shape:\", data.shape)\n",
        "        print(\"Metadata Sample:\", metadata)\n",
        "        break\n",
        "except Exception as e:\n",
        "    print(f\"Error in Multiome GEX DataLoader: {e}\")"
      ],
      "metadata": {
        "id": "VC4NSSCcyu82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d553377e-b5fd-4f41-a039-2575dc288c9f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing Multiome GEX dataset...\n",
            "Dense matrix for GEX found at /content/drive/MyDrive/ProjectX/GEX_dense.h5. Using saved file.\n",
            "Multiome GEX DataLoader:\n",
            "Batch 0:\n",
            "Data Shape: torch.Size([32, 129921])\n",
            "Metadata Sample: {'GEX_pct_counts_mt': tensor([0.6472, 0.7900, 0.2937, 1.2965, 1.0743, 1.7241, 0.2059, 0.0418, 0.7239,\n",
            "        0.4510, 0.7623, 0.1028, 0.2642, 2.1480, 1.4865, 0.0000, 0.6875, 0.4544,\n",
            "        1.1917, 0.0000, 0.0000, 4.4944, 0.0000, 0.2690, 0.0530, 0.0718, 0.2328,\n",
            "        0.3130, 0.0000, 5.6829, 0.0000, 0.1160], dtype=torch.float64), 'GEX_n_counts': tensor([ 618., 2405.,  681., 2314., 2141., 1160., 1943., 4781., 4144.,  887.,\n",
            "        1443.,  973.,  757.,  838., 1480.,  872., 1600., 7262., 1007., 2535.,\n",
            "        1854.,  623., 1479., 3718., 1888., 2787.,  859., 1278.,  984., 3097.,\n",
            "        4614., 2587.], dtype=torch.float64), 'GEX_n_genes': tensor([ 494, 1575,  549, 1425, 1242,  812, 1267, 2500, 1060,  616, 1002,  682,\n",
            "         554,  603, 1070,  706, 1073, 3177,  764, 1564, 1181,  479, 1042, 2044,\n",
            "        1307, 1561,  377,  908,  650, 1808, 1172, 1633]), 'GEX_size_factors': tensor([0.4429, 2.0235, 0.1673, 0.8135, 0.7197, 0.3549, 0.5948, 1.8844, 0.4875,\n",
            "        0.6765, 0.4287, 0.2858, 0.5629, 0.5563, 0.5873, 0.4879, 1.1705, 2.7745,\n",
            "        0.3974, 0.6221, 0.9824, 0.5132, 0.6516, 0.9891, 1.0638, 0.7170, 0.0934,\n",
            "        0.3821, 0.3357, 2.3741, 0.4420, 0.6416], dtype=torch.float64), 'GEX_phase': ['G2M', 'S', 'S', 'G2M', 'S', 'G2M', 'S', 'S', 'G2M', 'G2M', 'G2M', 'G2M', 'G2M', 'S', 'S', 'G2M', 'G2M', 'S', 'S', 'G2M', 'S', 'S', 'G2M', 'G2M', 'S', 'G2M', 'G2M', 'S', 'S', 'S', 'G2M', 'S'], 'ATAC_nCount_peaks': tensor([ 4257.,  1644.,  4109.,  7555.,  6407.,  5918.,  4024., 14006.,  9985.,\n",
            "         7409.,  6108.,  5506.,  2130.,  3963.,  2489.,  6916.,  5996., 29054.,\n",
            "         6357., 11312.,  5794.,  4922.,  6073.,  9478.,  2595.,  8017.,  2954.,\n",
            "          568.,  2139., 23593.,  3534.,  2085.], dtype=torch.float64), 'ATAC_atac_fragments': tensor([ 5077,  2956,  4056, 12339,  9022,  8785,  4017, 38698, 17876,  8869,\n",
            "         8759,  8442,  2713,  4197,  4934, 13946,  7053, 39392,  8565, 14752,\n",
            "        10706,  4165, 14768, 14666,  4208, 15966,  4022,   390,  5688, 28096,\n",
            "         3814,  2742]), 'ATAC_reads_in_peaks_frac': tensor([0.8385, 0.5562, 1.0131, 0.6123, 0.7102, 0.6736, 1.0017, 0.3619, 0.5586,\n",
            "        0.8354, 0.6973, 0.6522, 0.7851, 0.9442, 0.5045, 0.4959, 0.8501, 0.7376,\n",
            "        0.7422, 0.7668, 0.5412, 1.1818, 0.4112, 0.6463, 0.6167, 0.5021, 0.7345,\n",
            "        1.4564, 0.3761, 0.8397, 0.9266, 0.7604], dtype=torch.float64), 'ATAC_blacklist_fraction': tensor([0.0038, 0.0000, 0.0005, 0.0011, 0.0003, 0.0020, 0.0010, 0.0011, 0.0006,\n",
            "        0.0011, 0.0029, 0.0011, 0.0019, 0.0020, 0.0024, 0.0023, 0.0020, 0.0013,\n",
            "        0.0003, 0.0004, 0.0007, 0.0016, 0.0000, 0.0002, 0.0008, 0.0027, 0.0014,\n",
            "        0.0000, 0.0000, 0.0028, 0.0000, 0.0029], dtype=torch.float64), 'ATAC_nucleosome_signal': tensor([0.9489, 1.2138, 0.7066, 0.6540, 0.8151, 0.8642, 1.2384, 0.8339, 0.5736,\n",
            "        0.7841, 0.7362, 1.3866, 1.2465, 0.7033, 1.2554, 1.4030, 0.9757, 0.7495,\n",
            "        0.6492, 1.0209, 1.1978, 0.4517, 0.9001, 1.3456, 1.1016, 1.6957, 1.0196,\n",
            "        0.7292, 0.9978, 0.6797, 0.5731, 1.0945], dtype=torch.float64), 'cell_type': ['CD4+ T naive', 'MK/E prog', 'ILC', 'CD8+ T', 'CD4+ T naive', 'B1 B', 'HSC', 'Lymph prog', 'Erythroblast', 'Erythroblast', 'Naive CD20+ B', 'Naive CD20+ B', 'CD4+ T naive', 'CD8+ T', 'CD4+ T activated', 'CD14+ Mono', 'CD16+ Mono', 'CD4+ T naive', 'CD4+ T activated', 'CD14+ Mono', 'CD14+ Mono', 'CD4+ T activated', 'CD8+ T naive', 'CD16+ Mono', 'CD4+ T naive', 'CD14+ Mono', 'Normoblast', 'NK', 'Erythroblast', 'Lymph prog', 'Erythroblast', 'CD14+ Mono'], 'batch': ['s4d1', 's4d1', 's2d5', 's1d3', 's1d2', 's1d1', 's2d4', 's3d10', 's1d3', 's4d8', 's1d1', 's2d1', 's4d1', 's4d8', 's1d2', 's3d3', 's4d1', 's1d2', 's1d2', 's2d5', 's3d3', 's4d9', 's3d10', 's2d1', 's3d3', 's2d1', 's2d1', 's2d4', 's3d10', 's4d9', 's2d5', 's2d5'], 'ATAC_pseudotime_order': tensor([   nan, 0.5376,    nan,    nan,    nan,    nan, 0.4104,    nan, 0.9459,\n",
            "        0.9445,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.8939,\n",
            "           nan, 0.8080,    nan, 0.5625,    nan], dtype=torch.float64), 'GEX_pseudotime_order': tensor([   nan, 0.3401,    nan,    nan,    nan,    nan, 0.0929,    nan, 0.9840,\n",
            "        0.5321,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.9357,\n",
            "           nan, 0.7515,    nan, 0.8393,    nan], dtype=torch.float64), 'Samplename': ['site4_donor1_multiome', 'site4_donor1_multiome', 'site2_donor5_multiome', 'site1_donor3_multiome', 'site1_donor2_multiome', 'site1_donor1_multiome', 'site2_donor4_multiome', 'site3_donor10_multiome', 'site1_donor3_multiome', 'site4_donor8_multiome', 'site1_donor1_multiome', 'site2_donor1_multiome', 'site4_donor1_multiome', 'site4_donor8_multiome', 'site1_donor2_multiome', 'site3_donor3_multiome', 'site4_donor1_multiome', 'site1_donor2_multiome', 'site1_donor2_multiome', 'site2_donor5_multiome', 'site3_donor3_multiome', 'site4_donor9_multiome', 'site3_donor10_multiome', 'site2_donor1_multiome', 'site3_donor3_multiome', 'site2_donor1_multiome', 'site2_donor1_multiome', 'site2_donor4_multiome', 'site3_donor10_multiome', 'site4_donor9_multiome', 'site2_donor5_multiome', 'site2_donor5_multiome'], 'Site': ['site4', 'site4', 'site2', 'site1', 'site1', 'site1', 'site2', 'site3', 'site1', 'site4', 'site1', 'site2', 'site4', 'site4', 'site1', 'site3', 'site4', 'site1', 'site1', 'site2', 'site3', 'site4', 'site3', 'site2', 'site3', 'site2', 'site2', 'site2', 'site3', 'site4', 'site2', 'site2'], 'DonorNumber': ['donor1', 'donor1', 'donor5', 'donor3', 'donor2', 'donor1', 'donor4', 'donor10', 'donor3', 'donor8', 'donor1', 'donor1', 'donor1', 'donor8', 'donor2', 'donor3', 'donor1', 'donor2', 'donor2', 'donor5', 'donor3', 'donor9', 'donor10', 'donor1', 'donor3', 'donor1', 'donor1', 'donor4', 'donor10', 'donor9', 'donor5', 'donor5'], 'Modality': ['multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome'], 'VendorLot': tensor([3054455, 3054455, 3054457, 3054716, 3054129, 3054455, 3051683, 3064467,\n",
            "        3054716, 3051675, 3054455, 3054455, 3054455, 3051675, 3054129, 3054716,\n",
            "        3054455, 3054129, 3054129, 3054457, 3054716, 3061608, 3064467, 3054455,\n",
            "        3054716, 3054455, 3054455, 3051683, 3064467, 3061608, 3054457, 3054457]), 'DonorID': tensor([15078, 15078, 16710, 18303, 10886, 15078, 12710, 28483, 18303, 19593,\n",
            "        15078, 15078, 15078, 19593, 10886, 18303, 15078, 10886, 10886, 16710,\n",
            "        18303, 13272, 28483, 15078, 18303, 15078, 15078, 12710, 28483, 13272,\n",
            "        16710, 16710]), 'DonorAge': tensor([34, 34, 40, 33, 35, 34, 27, 21, 33, 31, 34, 34, 34, 31, 35, 33, 34, 35,\n",
            "        35, 40, 33, 35, 21, 34, 33, 34, 34, 27, 21, 35, 40, 40]), 'DonorBMI': tensor([24.8000, 24.8000, 27.8000, 24.0000, 28.6000, 24.8000, 32.1000, 26.7000,\n",
            "        24.0000, 32.6000, 24.8000, 24.8000, 24.8000, 32.6000, 28.6000, 24.0000,\n",
            "        24.8000, 28.6000, 28.6000, 27.8000, 24.0000, 31.0000, 26.7000, 24.8000,\n",
            "        24.0000, 24.8000, 24.8000, 32.1000, 26.7000, 31.0000, 27.8000, 27.8000],\n",
            "       dtype=torch.float64), 'DonorBloodType': ['B-', 'B-', 'O+', 'O+', 'B+', 'B-', 'O+', 'O+', 'O+', 'A+', 'B-', 'B-', 'B-', 'A+', 'B+', 'O+', 'B-', 'B+', 'B+', 'O+', 'O+', 'O+', 'O+', 'B-', 'O+', 'B-', 'B-', 'O+', 'O+', 'O+', 'O+', 'O+'], 'DonorRace': ['White', 'White', 'White', 'Asian', 'Asian', 'White', 'White', 'Other Race', 'Asian', 'Black or African American', 'White', 'White', 'White', 'Black or African American', 'Asian', 'Asian', 'White', 'Asian', 'Asian', 'White', 'Asian', 'Other Race', 'Other Race', 'White', 'Asian', 'White', 'White', 'White', 'Other Race', 'Other Race', 'White', 'White'], 'Ethnicity': ['HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO'], 'DonorGender': ['Male', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Female', 'Female', 'Female', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Female', 'Female'], 'QCMeds': ['False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False'], 'DonorSmoker': ['Nonsmoker', 'Nonsmoker', 'Smoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Smoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Smoker', 'Smoker']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ATAC-seq"
      ],
      "metadata": {
        "id": "r3PX51gryfYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dataset for Multiome (ATAC modality)\n",
        "print(\"\\nInitializing Multiome ATAC dataset...\")\n",
        "try:\n",
        "    multiome_dataset_atac = MultiModalDataset(adata_multiome, modality=\"ATAC\", use_gpu=False, chunk_size=10000)\n",
        "    multiome_atac_loader = DataLoader(multiome_dataset_atac, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Test Multiome ATAC DataLoader\n",
        "    print(\"Multiome ATAC DataLoader:\")\n",
        "    for batch_idx, (data, metadata) in enumerate(multiome_atac_loader):\n",
        "        print(f\"Batch {batch_idx}:\")\n",
        "        print(\"Data Shape:\", data.shape)\n",
        "        print(\"Metadata Sample:\", metadata)\n",
        "        break\n",
        "except Exception as e:\n",
        "    print(f\"Error in Multiome ATAC DataLoader: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc3tt8egy6S5",
        "outputId": "f6956ba6-72a5-4579-bdf2-eef1a24fd1ee"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing Multiome ATAC dataset...\n",
            "Dense matrix for ATAC found at /content/drive/MyDrive/ProjectX/ATAC_dense.h5. Using saved file.\n",
            "Multiome ATAC DataLoader:\n",
            "Batch 0:\n",
            "Data Shape: torch.Size([32, 129921])\n",
            "Metadata Sample: {'GEX_pct_counts_mt': tensor([2.1098, 3.0392, 0.1264, 1.0056, 0.0354, 0.2554, 1.5446, 1.8405, 0.0000,\n",
            "        0.0000, 4.0214, 0.0000, 1.5402, 0.3175, 0.0000, 0.0572, 1.2195, 0.0000,\n",
            "        0.0000, 2.8213, 0.0000, 4.9808, 0.2882, 0.6223, 0.0000, 1.8182, 0.9080,\n",
            "        1.0944, 1.1971, 0.5672, 1.1788, 0.3643], dtype=torch.float64), 'GEX_n_counts': tensor([2986., 1020., 2374., 2685., 2826.,  783., 2007.,  489.,  883., 1773.,\n",
            "         373., 1208., 1753., 1260., 2427., 1747.,  410., 1350., 1458.,  319.,\n",
            "         489.,  261., 1041., 1607., 1028.,  330., 1652.,  731., 1253., 3350.,\n",
            "         509., 1098.], dtype=torch.float64), 'GEX_n_genes': tensor([1857,  764, 1226, 1492, 1534,  606, 1168,  378,  668, 1127,  325,  834,\n",
            "        1048,  891, 1471, 1147,  366,  981, 1032,  262,  433,  235,  737, 1053,\n",
            "         782,  281, 1026,  579,  854, 1565,  410,  810]), 'GEX_size_factors': tensor([2.1787, 0.8438, 0.5648, 0.8728, 0.7939, 0.2053, 0.6977, 0.3957, 0.2372,\n",
            "        0.4667, 0.2205, 0.3288, 0.5674, 0.3819, 0.9915, 0.4933, 0.1992, 0.4088,\n",
            "        0.4107, 0.0990, 0.4176, 0.2164, 0.2822, 0.4469, 0.2728, 0.1927, 0.4672,\n",
            "        0.5434, 0.8167, 0.9841, 0.3439, 0.7552], dtype=torch.float64), 'GEX_phase': ['S', 'G1', 'G2M', 'G2M', 'S', 'G2M', 'G2M', 'S', 'G2M', 'G2M', 'S', 'G2M', 'S', 'G2M', 'G2M', 'G2M', 'S', 'G2M', 'G2M', 'S', 'G2M', 'G2M', 'G2M', 'G2M', 'G2M', 'G2M', 'G2M', 'G2M', 'G2M', 'G2M', 'S', 'G2M'], 'ATAC_nCount_peaks': tensor([17066.,  6250.,  5978.,  7420.,   118.,  2031.,  6073., 12007.,  4776.,\n",
            "         8075.,  8130.,  6735.,  8383.,  7236.,  3068.,  4234.,  2642.,  1648.,\n",
            "         5497.,  2115., 11632.,  3168.,  5373.,  7736.,  7891.,  4237.,  3912.,\n",
            "         2799.,  4465., 10216.,  1149.,  5044.], dtype=torch.float64), 'ATAC_atac_fragments': tensor([24905,  5843,  5695, 14005,  1381,  2075,  7979, 10356,  5614, 11899,\n",
            "         9283, 55081, 11131,  6109,  7477,  4488,  6024, 12036,  6417,  3003,\n",
            "        12760,  3361,  4227, 10522, 86731,  4675,  6649,  3138,  4739, 14227,\n",
            "         2029,  5432]), 'ATAC_reads_in_peaks_frac': tensor([0.6852, 1.0697, 1.0497, 0.5298, 0.0854, 0.9788, 0.7611, 1.1594, 0.8507,\n",
            "        0.6786, 0.8758, 0.1223, 0.7531, 1.1845, 0.4103, 0.9434, 0.4386, 0.1369,\n",
            "        0.8566, 0.7043, 0.9116, 0.9426, 1.2711, 0.7352, 0.0910, 0.9063, 0.5884,\n",
            "        0.8920, 0.9422, 0.7181, 0.5663, 0.9286], dtype=torch.float64), 'ATAC_blacklist_fraction': tensor([0.0014, 0.0048, 0.0017, 0.0022, 0.0000, 0.0010, 0.0008, 0.0007, 0.0017,\n",
            "        0.0002, 0.0005, 0.0006, 0.0016, 0.0012, 0.0000, 0.0000, 0.0061, 0.0000,\n",
            "        0.0007, 0.0009, 0.0015, 0.0013, 0.0000, 0.0034, 0.0013, 0.0009, 0.0010,\n",
            "        0.0021, 0.0027, 0.0006, 0.0000, 0.0004], dtype=torch.float64), 'ATAC_nucleosome_signal': tensor([0.7992, 0.6347, 0.8689, 0.7804, 1.5238, 0.8981, 0.6378, 0.5738, 0.8806,\n",
            "        1.0501, 0.8866, 1.4132, 0.7821, 1.0384, 0.9363, 0.8372, 1.6746, 1.0440,\n",
            "        1.0427, 0.5928, 0.6480, 0.7190, 0.8122, 0.8126, 1.5149, 0.8436, 0.5875,\n",
            "        0.8507, 0.7198, 0.7617, 1.5429, 0.8243], dtype=torch.float64), 'cell_type': ['G/M prog', 'HSC', 'Erythroblast', 'CD8+ T', 'Erythroblast', 'ILC', 'CD8+ T', 'Transitional B', 'NK', 'CD14+ Mono', 'NK', 'CD14+ Mono', 'CD8+ T', 'CD4+ T naive', 'CD14+ Mono', 'CD8+ T', 'CD14+ Mono', 'NK', 'NK', 'ILC', 'Transitional B', 'CD8+ T', 'Naive CD20+ B', 'Naive CD20+ B', 'CD14+ Mono', 'CD8+ T', 'CD8+ T', 'CD4+ T activated', 'Naive CD20+ B', 'CD4+ T naive', 'Proerythroblast', 'CD14+ Mono'], 'batch': ['s4d8', 's4d9', 's2d4', 's1d3', 's3d7', 's2d5', 's1d2', 's4d9', 's2d5', 's3d6', 's4d8', 's3d7', 's1d2', 's2d4', 's3d10', 's2d5', 's4d8', 's3d7', 's2d5', 's1d2', 's4d1', 's4d8', 's2d4', 's1d1', 's3d7', 's4d8', 's1d3', 's4d1', 's4d8', 's1d2', 's4d1', 's4d1'], 'ATAC_pseudotime_order': tensor([   nan, 0.3054, 0.7455,    nan, 0.3693,    nan,    nan,    nan,    nan,\n",
            "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "           nan,    nan,    nan, 0.5902,    nan], dtype=torch.float64), 'GEX_pseudotime_order': tensor([   nan, 0.0392, 0.9262,    nan, 0.8684,    nan,    nan,    nan,    nan,\n",
            "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
            "           nan,    nan,    nan, 0.4481,    nan], dtype=torch.float64), 'Samplename': ['site4_donor8_multiome', 'site4_donor9_multiome', 'site2_donor4_multiome', 'site1_donor3_multiome', 'site3_donor7_multiome', 'site2_donor5_multiome', 'site1_donor2_multiome', 'site4_donor9_multiome', 'site2_donor5_multiome', 'site3_donor6_multiome', 'site4_donor8_multiome', 'site3_donor7_multiome', 'site1_donor2_multiome', 'site2_donor4_multiome', 'site3_donor10_multiome', 'site2_donor5_multiome', 'site4_donor8_multiome', 'site3_donor7_multiome', 'site2_donor5_multiome', 'site1_donor2_multiome', 'site4_donor1_multiome', 'site4_donor8_multiome', 'site2_donor4_multiome', 'site1_donor1_multiome', 'site3_donor7_multiome', 'site4_donor8_multiome', 'site1_donor3_multiome', 'site4_donor1_multiome', 'site4_donor8_multiome', 'site1_donor2_multiome', 'site4_donor1_multiome', 'site4_donor1_multiome'], 'Site': ['site4', 'site4', 'site2', 'site1', 'site3', 'site2', 'site1', 'site4', 'site2', 'site3', 'site4', 'site3', 'site1', 'site2', 'site3', 'site2', 'site4', 'site3', 'site2', 'site1', 'site4', 'site4', 'site2', 'site1', 'site3', 'site4', 'site1', 'site4', 'site4', 'site1', 'site4', 'site4'], 'DonorNumber': ['donor8', 'donor9', 'donor4', 'donor3', 'donor7', 'donor5', 'donor2', 'donor9', 'donor5', 'donor6', 'donor8', 'donor7', 'donor2', 'donor4', 'donor10', 'donor5', 'donor8', 'donor7', 'donor5', 'donor2', 'donor1', 'donor8', 'donor4', 'donor1', 'donor7', 'donor8', 'donor3', 'donor1', 'donor8', 'donor2', 'donor1', 'donor1'], 'Modality': ['multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome', 'multiome'], 'VendorLot': tensor([3051675, 3061608, 3051683, 3054716, 3054734, 3054457, 3054129, 3061608,\n",
            "        3054457, 3059371, 3051675, 3054734, 3054129, 3051683, 3064467, 3054457,\n",
            "        3051675, 3054734, 3054457, 3054129, 3054455, 3051675, 3051683, 3054455,\n",
            "        3054734, 3051675, 3054716, 3054455, 3051675, 3054129, 3054455, 3054455]), 'DonorID': tensor([19593, 13272, 12710, 18303, 11466, 16710, 10886, 13272, 16710, 28045,\n",
            "        19593, 11466, 10886, 12710, 28483, 16710, 19593, 11466, 16710, 10886,\n",
            "        15078, 19593, 12710, 15078, 11466, 19593, 18303, 15078, 19593, 10886,\n",
            "        15078, 15078]), 'DonorAge': tensor([31, 35, 27, 33, 22, 40, 35, 35, 40, 36, 31, 22, 35, 27, 21, 40, 31, 22,\n",
            "        40, 35, 34, 31, 27, 34, 22, 31, 33, 34, 31, 35, 34, 34]), 'DonorBMI': tensor([32.6000, 31.0000, 32.1000, 24.0000, 31.5000, 27.8000, 28.6000, 31.0000,\n",
            "        27.8000, 23.8000, 32.6000, 31.5000, 28.6000, 32.1000, 26.7000, 27.8000,\n",
            "        32.6000, 31.5000, 27.8000, 28.6000, 24.8000, 32.6000, 32.1000, 24.8000,\n",
            "        31.5000, 32.6000, 24.0000, 24.8000, 32.6000, 28.6000, 24.8000, 24.8000],\n",
            "       dtype=torch.float64), 'DonorBloodType': ['A+', 'O+', 'O+', 'O+', 'A+', 'O+', 'B+', 'O+', 'O+', 'A+', 'A+', 'A+', 'B+', 'O+', 'O+', 'O+', 'A+', 'A+', 'O+', 'B+', 'B-', 'A+', 'O+', 'B-', 'A+', 'A+', 'O+', 'B-', 'A+', 'B+', 'B-', 'B-'], 'DonorRace': ['Black or African American', 'Other Race', 'White', 'Asian', 'Asian', 'White', 'Asian', 'Other Race', 'White', 'Other Race', 'Black or African American', 'Asian', 'Asian', 'White', 'Other Race', 'White', 'Black or African American', 'Asian', 'White', 'Asian', 'White', 'Black or African American', 'White', 'White', 'Asian', 'Black or African American', 'Asian', 'White', 'Black or African American', 'Asian', 'White', 'White'], 'Ethnicity': ['NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'NOT HISPANIC OR LATINO', 'HISPANIC OR LATINO', 'HISPANIC OR LATINO'], 'DonorGender': ['Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Female', 'Male', 'Female', 'Female', 'Male', 'Female', 'Female', 'Male', 'Female', 'Female', 'Male', 'Female', 'Female', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male'], 'QCMeds': ['False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'True', 'False', 'False'], 'DonorSmoker': ['Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Smoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker', 'Nonsmoker']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Under Construction] CITE-seq"
      ],
      "metadata": {
        "id": "XG9lS79myY3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cite_dataset = MultiModalDataset(adata_cite, modality=\"ADT\", use_gpu=False, chunk_size=10000)\n",
        "\n",
        "# Debug CITE-seq DataLoader\n",
        "cite_loader = DataLoader(cite_dataset, batch_size=32, shuffle=True)\n",
        "for idx, (data, metadata) in enumerate(cite_loader):\n",
        "    print(f\"Batch {idx}: Data shape {data.shape}, Metadata shape {len(metadata)}\")\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "W-imUn0V7CCM",
        "outputId": "17bfc89b-7261-4d4c-b764-88f02b570b16"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADT is already dense. Using in-memory data.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "23927",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 23927",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-c7795d87bc82>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Debug CITE-seq DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcite_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcite_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcite_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {idx}: Data shape {data.shape}, Metadata shape {len(metadata)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-662d5116a730>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Access dense data from disk or memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mdata_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Use in-memory dense data directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 23927"
          ]
        }
      ]
    }
  ]
}