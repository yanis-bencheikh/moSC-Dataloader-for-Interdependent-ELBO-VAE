# -*- coding: utf-8 -*-
"""moSC_dataloader_for_interdependent_ELBO_VAE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sAVU9T2LCVS1EKBeaep4snoNdzmID79T

# Download & Uncompress Datasets
"""

# from google.colab import drive
# import requests
# import gzip
# import shutil
# import os

# # Mount Google Drive
# drive.mount('/content/drive')

# # Define URLs and file paths
# urls = {
#     "CITE-seq": "https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE194122&format=file&file=GSE194122%5Fopenproblems%5Fneurips2021%5Fcite%5FBMMC%5Fprocessed%2Eh5ad%2Egz",
#     "Multiome": "https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE194122&format=file&file=GSE194122%5Fopenproblems%5Fneurips2021%5Fmultiome%5FBMMC%5Fprocessed%2Eh5ad%2Egz"
# }

# output_dir = "/content/drive/MyDrive/ProjectX/data"
# os.makedirs(output_dir, exist_ok=True)

# for name, url in urls.items():
#     output_gz = os.path.join(output_dir, f"{name}.h5ad.gz")
#     output_file = os.path.join(output_dir, f"{name}.h5ad")

#     # Download the dataset
#     print(f"Downloading {name} dataset...")
#     response = requests.get(url, stream=True)
#     with open(output_gz, 'wb') as f:
#         for chunk in response.iter_content(chunk_size=1024):
#             if chunk:
#                 f.write(chunk)
#     print(f"{name} dataset saved to {output_gz}")

#     # Decompress the file
#     print(f"Decompressing {name} dataset...")
#     with gzip.open(output_gz, 'rb') as f_in:
#         with open(output_file, 'wb') as f_out:
#             shutil.copyfileobj(f_in, f_out)
#     print(f"{name} dataset decompressed and saved to {output_file}")

#     # Optional: Delete the .gz file to save space
#     os.remove(output_gz)
#     print(f"Compressed file {output_gz} deleted.\n")

"""# Installs & Loading Datasets"""

from google.colab import drive
drive.mount('/content/drive')

!pip install scanpy torch numpy pandas

import scanpy as sc
import torch
from torch.utils.data import Dataset, DataLoader

# Define the paths to the datasets
cite_path = "/content/drive/MyDrive/ProjectX/data/CITE-seq.h5ad"
multiome_path = "/content/drive/MyDrive/ProjectX/data/Multiome.h5ad"

# Load the datasets using scanpy
adata_cite = sc.read_h5ad(cite_path)
adata_multiome = sc.read_h5ad(multiome_path)

# Display basic information about the datasets
print("CITE-seq Dataset:")
print(adata_cite)
print("\nMultiome Dataset:")
print(adata_multiome)

"""# Dataloader

## Class
"""

import os
import scipy.sparse
import h5py
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
import gc

class MultiModalDataset(Dataset):
    def __init__(self, adata, modality="GEX", use_gpu=False, chunk_size=5000, save_dir="/content/drive/MyDrive/ProjectX/"):
        """
        Initialize the dataset with sparse-to-dense conversion handled efficiently.
        Args:
            adata: AnnData object containing the data.
            modality: The modality to extract ("GEX", "ATAC", "ADT").
            use_gpu: Whether to use GPU for sparse-to-dense conversion.
            chunk_size: Number of rows to process at a time if converting in chunks.
            save_dir: Directory where dense matrices will be saved/loaded.
        """
        self.adata = adata
        self.modality = modality
        self.use_gpu = use_gpu
        self.chunk_size = chunk_size
        self.save_dir = save_dir

        # Define the file path for the dense matrix
        self.data_path = os.path.join(self.save_dir, f"{modality}_dense.h5")
        self.data = None  # Initialize self.data for direct use of dense data

        # Check if a dense matrix file already exists
        if os.path.exists(self.data_path):
            print(f"Dense matrix for {modality} found at {self.data_path}. Using saved file.")
        else:
            # Check if the data is sparse or dense in-memory
            if modality in self.adata.layers:
                if scipy.sparse.issparse(self.adata.layers[modality]):
                    print(f"Converting sparse matrix for {modality} to dense.")
                    self.data_path = self._convert_sparse_to_dense_and_save(
                        self.adata.layers[modality], self.data_path, modality
                    )
                else:
                    print(f"{modality} is already dense. Using in-memory data.")
                    self.data = self.adata.layers[modality]  # Use dense matrix directly
            elif modality in self.adata.obsm:
                if scipy.sparse.issparse(self.adata.obsm[modality]):
                    print(f"Converting sparse matrix for {modality} to dense.")
                    self.data_path = self._convert_sparse_to_dense_and_save(
                        self.adata.obsm[modality], self.data_path, modality
                    )
                else:
                    print(f"{modality} is already dense. Using in-memory data.")
                    self.data = self.adata.obsm[modality]  # Use dense matrix directly
            else:
                raise ValueError(f"Unsupported modality: {modality}")

        # Initialize metadata
        self.metadata = self.adata.obs.reset_index(drop=True)

        # Validate alignment of metadata with data if dense data is used directly
        if self.data is not None:
            assert self.data.shape[0] == self.metadata.shape[0], (
                f"Data and metadata mismatch: {self.data.shape[0]} rows in data, "
                f"{self.metadata.shape[0]} rows in metadata."
            )

    def _convert_sparse_to_dense_and_save(self, sparse_matrix, output_file, dataset_name):
        """
        Convert a sparse matrix to dense incrementally, save to HDF5, and return the file path.
        """
        print(f"Saving dense matrix for {dataset_name} to {output_file}")
        self.sparse_to_dense_in_chunks_with_save(sparse_matrix, output_file, dataset_name, self.chunk_size)
        print(f"Dense matrix saved to {output_file}.")
        return output_file  # Return the file path

    @staticmethod
    def sparse_to_dense_in_chunks_with_save(sparse_matrix, output_file, dataset_name, chunk_size):
        """
        Convert a sparse matrix to dense in chunks and save it to an HDF5 file.
        """
        n_rows, n_cols = sparse_matrix.shape
        with h5py.File(output_file, "w") as f:
            dset = f.create_dataset(dataset_name, shape=(n_rows, n_cols), dtype=np.float32)

            for start in range(0, n_rows, chunk_size):
                end = min(start + chunk_size, n_rows)
                print(f"Processing rows {start} to {end}")
                dense_chunk = sparse_matrix[start:end, :].toarray()
                dset[start:end, :] = dense_chunk  # Write the chunk to disk
                del dense_chunk  # Delete the dense chunk from memory
                gc.collect()  # Trigger garbage collection
            print(f"Matrix saved to {output_file} as dataset '{dataset_name}'.")

    def __len__(self):
        return self.metadata.shape[0]

    def __getitem__(self, idx):
        """
        Retrieve a single data point and its metadata.
        """
        if idx >= len(self):
            raise IndexError(f"Index {idx} is out of range for dataset of size {len(self)}")

        # Access dense data from disk or memory
        if self.data is not None:
            data_sample = self.data[idx]  # Use in-memory dense data directly
        else:
            with h5py.File(self.data_path, "r") as f:
                data_sample = f[self.modality][idx, :]  # Access data from disk

        metadata_sample = self.metadata.iloc[idx]
        return torch.tensor(data_sample, dtype=torch.float32), metadata_sample.to_dict()

"""## GEX"""

# Initialize dataset for Multiome (GEX modality)
print("\nInitializing Multiome GEX dataset...")
try:
    multiome_dataset_gex = MultiModalDataset(adata_multiome, modality="GEX", use_gpu=False, chunk_size=10000)
    multiome_gex_loader = DataLoader(multiome_dataset_gex, batch_size=32, shuffle=True)

    # Test Multiome GEX DataLoader
    print("Multiome GEX DataLoader:")
    for batch_idx, (data, metadata) in enumerate(multiome_gex_loader):
        print(f"Batch {batch_idx}:")
        print("Data Shape:", data.shape)
        print("Metadata Sample:", metadata)
        break
except Exception as e:
    print(f"Error in Multiome GEX DataLoader: {e}")

"""## ATAC-seq"""

# Initialize dataset for Multiome (ATAC modality)
print("\nInitializing Multiome ATAC dataset...")
try:
    multiome_dataset_atac = MultiModalDataset(adata_multiome, modality="ATAC", use_gpu=False, chunk_size=10000)
    multiome_atac_loader = DataLoader(multiome_dataset_atac, batch_size=32, shuffle=True)

    # Test Multiome ATAC DataLoader
    print("Multiome ATAC DataLoader:")
    for batch_idx, (data, metadata) in enumerate(multiome_atac_loader):
        print(f"Batch {batch_idx}:")
        print("Data Shape:", data.shape)
        print("Metadata Sample:", metadata)
        break
except Exception as e:
    print(f"Error in Multiome ATAC DataLoader: {e}")

"""## CITE-seq"""

print("Available layers:", adata_cite.layers.keys())
print("Available obsm keys:", adata_cite.obsm.keys())

"""**CITE-seq dataloader is functional for every modality except "ADT_isotype_controls"**"""

# Ensure the ADT modality is correctly identified in MultiModalDataset
cite_dataset = MultiModalDataset(adata_cite, modality="ADT_X_umap", use_gpu=False, chunk_size=10000)

# Debug CITE-seq DataLoader
cite_loader = DataLoader(cite_dataset, batch_size=32, shuffle=True)
for idx, (data, metadata) in enumerate(cite_loader):
    print(f"Batch {idx}: Data shape {data.shape}, Metadata shape {len(metadata)}")
    break